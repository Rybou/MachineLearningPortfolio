{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening training and test files..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data frames..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from data frames..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 4000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 6000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 8000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 12000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 14000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 16000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 18000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 20000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 22000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 24000 comments"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import ClassifierDataPrepper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "dataPath = \"./\"\n",
    "trainingDataPath = dataPath + \"train/\"\n",
    "positiveTrainingDataPath = trainingDataPath + \"pos/\"\n",
    "negativeTrainingDataPath = trainingDataPath + \"neg/\"\n",
    "# testDataPath = dataPath + \"test/\"\n",
    "# positiveTrainingDataPath = trainingDataPath + \"pos_small/\"\n",
    "# negativeTrainingDataPath = trainingDataPath + \"neg_small/\"\n",
    "testDataPath = None\n",
    "\n",
    "print(\"Opening training and test files...\")\n",
    "cdp = ClassifierDataPrepper.ClassifierDataPrepper(positiveTrainingDataPath, negativeTrainingDataPath, testDataPath)\n",
    "\n",
    "print(\"Preparing data frames...\")\n",
    "X, Y = cdp.getXYlabeled()\n",
    "\n",
    "print(\"Extracting features from data frames...\")\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp_spacy = spacy.load('en', disable=['parser', 'ner'])\n",
    "X_lem = []\n",
    "idx = 0\n",
    "for comment in X:\n",
    "    comment = cdp.cleanhtml(comment)\n",
    "    sentences = nltk.sent_tokenize(comment)  # this gives us a list of sentences\n",
    "    # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "    comment_lem = []\n",
    "    for sentence in sentences:\n",
    "        sentence_lem_tokens = nlp_spacy(sentence)\n",
    "        sentence_lem = \" \".join([token.lemma_ for token in sentence_lem_tokens])\n",
    "        comment_lem.append(sentence_lem)\n",
    "\n",
    "    comment_lem = \" \".join(comment_lem)\n",
    "    # print(\"Comment:\")\n",
    "    # print(comment)\n",
    "    # print(\"Lemmatized Comment:\")\n",
    "    # print(comment_lem)\n",
    "    X_lem.append(comment_lem)\n",
    "    idx += 1\n",
    "    if not idx % 2000:\n",
    "        print(\"processed {} comments\".format(idx))\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 319616)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# vect = CountVectorizer(min_df=1, ngram_range=(1, 2), binary=False)\n",
    "# vect = CountVectorizer(ngram_range=(1, 2))\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1, 4))\n",
    "# vect = TfidfVectorizer(min_df=1, ngram_range=(1, 2))\n",
    "\n",
    "# learn the vocabularies from training data for each vector type\n",
    "vect.fit(X_lem)\n",
    "\n",
    "# transform training data\n",
    "X_lem_token = vect.transform(X_lem)\n",
    "\n",
    "# normalize the data (scale it down to 0 -> 1)\n",
    "X_lem_token = preprocessing.normalize(X_lem_token, norm='l2')\n",
    "\n",
    "# model = MLPClassifier(solver='sgd', alpha=1e-5,\n",
    "#                       hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "print(X_lem_token.shape)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# model = MLPClassifier(hidden_layer_sizes=(X.shape[1], X.shape[1], X.shape[1]))\n",
    "# model = LogisticRegression()\n",
    "param_grid = {'C': [1, 10, 100]}\n",
    "# param_grid = {'C': [100, 150, 1000]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_lem_token, Y)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "# print(\"Best estimator: \", grid.best_estimator_)\n",
    "# param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "# print(\"Performing k-fold cross validation for our model...\")\n",
    "# kfoldscores = cross_val_score(logRegModel, X, Y, cv=5)\n",
    "# # print(scores)\n",
    "# print(\"Mean model accuracy = {}\".format(kfoldscores.mean()))\n",
    "\n",
    "\n",
    "# Split labelled data into train and validation sets\n",
    "# print(\"Splitting the data only once and measuring performance for our trained model...\")\n",
    "# X_train, X_validate, Y_train, Y_validate = train_test_split(X_lem_token, Y, test_size=0.2, random_state=0)\n",
    "# \n",
    "# model.fit(X_train, Y_train)\n",
    "# predictions = model.predict(X_validate)\n",
    "# accuracy = metrics.accuracy_score(Y_validate, predictions)\n",
    "# print(\"Model accuracy = {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
