{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening training and test files..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data frames..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the data only once and measuring performance for our trained model..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting comments into words and removing html tags..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that s what one of the girls said at the end     is the soccer game a metaphor for a qualifying game between the girls    or more broadly    a free thinking group    and the authority       to germany    means to a future that s of hope      it s one of the most unforgettable cinematic experience i ve ever had    despite the crude cinematography and plot    and mild over acting    though i like the cast    they re lovable and well above the expectation for amateurs       the ridiculous situation is well captured    i can feel the deep frustration being denied to a game    being female and a soccer fan    and i cannot stop thinking how to make a convincing disguise    i wonder why there s no women s section in which protection from dirty language and bad behavior can be provided    defeating the flawed reasons for the deny     the movie is very cleverly made    the amazing title    the filming during the actual game    the spontaneity    and various methods to put the viewers into the shoes of the characters    the game that s so important but inaccessible    not shown       the luring light and cheering sound from the stadium    the confinement of the van    and the uselessness of it when those inside connect with the celebrating crowds outside    i can feel the comfort coming from the radio    the drinks and the food    and of course    the kindness and consideration from each character to others    during the end credits    i am amused that no character has a name    he s just any    soldier    and she s just any    girl    or    sister      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!!!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import importlib\n",
    "import ClassifierDataPrepper\n",
    "importlib.reload(ClassifierDataPrepper)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "dataPath = \"./\"\n",
    "trainingDataPath = dataPath + \"train/\"\n",
    "\n",
    "positiveTrainingDataPath = trainingDataPath + \"pos/\"\n",
    "negativeTrainingDataPath = trainingDataPath + \"neg/\"\n",
    "testDataPath = dataPath + \"test/\"\n",
    "# positiveTrainingDataPath = trainingDataPath + \"pos_small/\"\n",
    "# negativeTrainingDataPath = trainingDataPath + \"neg_small/\"\n",
    "# testDataPath = dataPath + \"test_small/\"\n",
    "# testDataPath = None\n",
    "print(\"Opening training and test files...\")\n",
    "cdp = ClassifierDataPrepper.ClassifierDataPrepper(positiveTrainingDataPath, negativeTrainingDataPath, testDataPath)\n",
    "\n",
    "print(\"Preparing data frames...\")\n",
    "X, Y = cdp.getXYlabeled()\n",
    "Z = cdp.getXtest()\n",
    "\n",
    "print(\"Splitting the data only once and measuring performance for our trained model...\")\n",
    "X_train_pd, X_validate_pd, Y_train, Y_validate = train_test_split(X, Y, test_size=0.2, random_state=101)\n",
    "\n",
    "print(\"Splitting comments into words and removing html tags...\")\n",
    "X_train_words = []\n",
    "X_train = []\n",
    "for comment in X_train_pd:\n",
    "    comment = cdp.cleanhtml(comment).lower()\n",
    "    X_train.append(comment)\n",
    "    comment = comment.split()\n",
    "    X_train_words.append(comment)   \n",
    "print(X_train[0])    \n",
    "X_validate_words = [] \n",
    "X_validate = []\n",
    "for comment in X_validate_pd:\n",
    "    comment = cdp.cleanhtml(comment).lower()\n",
    "    X_validate.append(comment)\n",
    "    comment = comment.split()\n",
    "    X_validate_words.append(comment)    \n",
    "    \n",
    "Z_words = []\n",
    "for comment in Z:\n",
    "    comment = cdp.cleanhtml(comment)\n",
    "    comment = comment.lower().split()\n",
    "    Z_words.append(comment)\n",
    "    \n",
    "Y_words = []\n",
    "for label in Y_train:\n",
    "    Y_words.append(label)    \n",
    "for label in Y_validate:\n",
    "    Y_words.append(label)       \n",
    "    \n",
    "print(len(X_train_words))\n",
    "print(len(Y_words))\n",
    "print(len(Z_words))\n",
    "print(len(X_train))\n",
    "print(len(X_validate))\n",
    "print(\"done!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training doc2vec..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['that', 's', 'what', 'one', 'of', 'the', 'girls', 'said', 'at', 'the', 'end', 'is', 'the', 'soccer', 'game', 'a', 'metaphor', 'for', 'a', 'qualifying', 'game', 'between', 'the', 'girls', 'or', 'more', 'broadly', 'a', 'free', 'thinking', 'group', 'and', 'the', 'authority', 'to', 'germany', 'means', 'to', 'a', 'future', 'that', 's', 'of', 'hope', 'it', 's', 'one', 'of', 'the', 'most', 'unforgettable', 'cinematic', 'experience', 'i', 've', 'ever', 'had', 'despite', 'the', 'crude', 'cinematography', 'and', 'plot', 'and', 'mild', 'over', 'acting', 'though', 'i', 'like', 'the', 'cast', 'they', 're', 'lovable', 'and', 'well', 'above', 'the', 'expectation', 'for', 'amateurs', 'the', 'ridiculous', 'situation', 'is', 'well', 'captured', 'i', 'can', 'feel', 'the', 'deep', 'frustration', 'being', 'denied', 'to', 'a', 'game', 'being', 'female', 'and', 'a', 'soccer', 'fan', 'and', 'i', 'cannot', 'stop', 'thinking', 'how', 'to', 'make', 'a', 'convincing', 'disguise', 'i', 'wonder', 'why', 'there', 's', 'no', 'women', 's', 'section', 'in', 'which', 'protection', 'from', 'dirty', 'language', 'and', 'bad', 'behavior', 'can', 'be', 'provided', 'defeating', 'the', 'flawed', 'reasons', 'for', 'the', 'deny', 'the', 'movie', 'is', 'very', 'cleverly', 'made', 'the', 'amazing', 'title', 'the', 'filming', 'during', 'the', 'actual', 'game', 'the', 'spontaneity', 'and', 'various', 'methods', 'to', 'put', 'the', 'viewers', 'into', 'the', 'shoes', 'of', 'the', 'characters', 'the', 'game', 'that', 's', 'so', 'important', 'but', 'inaccessible', 'not', 'shown', 'the', 'luring', 'light', 'and', 'cheering', 'sound', 'from', 'the', 'stadium', 'the', 'confinement', 'of', 'the', 'van', 'and', 'the', 'uselessness', 'of', 'it', 'when', 'those', 'inside', 'connect', 'with', 'the', 'celebrating', 'crowds', 'outside', 'i', 'can', 'feel', 'the', 'comfort', 'coming', 'from', 'the', 'radio', 'the', 'drinks', 'and', 'the', 'food', 'and', 'of', 'course', 'the', 'kindness', 'and', 'consideration', 'from', 'each', 'character', 'to', 'others', 'during', 'the', 'end', 'credits', 'i', 'am', 'amused', 'that', 'no', 'character', 'has', 'a', 'name', 'he', 's', 'just', 'any', 'soldier', 'and', 'she', 's', 'just', 'any', 'girl', 'or', 'sister'], [0])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training doc2vec model..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split labelled data into train and validation sets\n",
    "print(\"training doc2vec...\")\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = []\n",
    "idx = 0\n",
    "for comment in X_train_words:\n",
    "    documents.append(TaggedDocument(comment, [idx]))\n",
    "    idx += 1\n",
    "    \n",
    "# for comment in X_validate_words:\n",
    "#     documents.append(TaggedDocument(comment, [idx]))\n",
    "#     idx += 1    \n",
    "\n",
    "print(documents[0])\n",
    "print(len(documents))\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(documents)\n",
    "\n",
    "print(\"Training doc2vec model...\")\n",
    "d2v_model = Doc2Vec(vector_size=400, min_count=2, epochs=30, workers=8)\n",
    "d2v_model.build_vocab(documents)\n",
    "d2v_model.train(documents, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "d2v_model.save(fname)\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Loading d2vecmodel from file\")\n",
    "# d2v_model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n",
    "# \n",
    "# print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vectors from data..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 0 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 2000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 4000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 6000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 8000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 10000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 12000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 14000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 16000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 18000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 0 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 2000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 4000 comments..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting vectors from data...\")\n",
    "\n",
    "X_train_d2v = []\n",
    "for i, comment in enumerate(X_train_words):\n",
    "    X_train_d2v.append(d2v_model.infer_vector(comment))\n",
    "    if not i % 2000:\n",
    "        print(\"parsed {} comments...\".format(i))\n",
    "    \n",
    "X_validate_d2v = []\n",
    "for i, comment in enumerate(X_validate_words):\n",
    "    X_validate_d2v.append(d2v_model.infer_vector(comment))\n",
    "    if not i % 2000:\n",
    "        print(\"parsed {} comments...\".format(i))    \n",
    "    \n",
    "# Z_d2v = []\n",
    "# for comment in Z_words:\n",
    "#     Z_d2v.append(d2v_model.infer_vector(comment))\n",
    "    \n",
    "# print(X_validate_d2v[0])\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding BOW"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 363024)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 363024)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 400)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 363424)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 363424)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print(\"Adding BOW\")\n",
    "\n",
    "vectCount = TfidfVectorizer(min_df=2, ngram_range=(1, 2))\n",
    "vectCount.fit(X_train)\n",
    "\n",
    "X_train_count = vectCount.transform(X_train)\n",
    "# print(X_train_count.shape)\n",
    "X_validate_count = vectCount.transform(X_validate)\n",
    "import numpy as np\n",
    "# X_count_train_np = np.asarray(X_train_count)\n",
    "# X_count_validate_np = np.asarray(X_validate_count)\n",
    "X_d2v_train_np = np.asarray(X_train_d2v)\n",
    "X_d2v_validate_np = np.asarray(X_validate_d2v)\n",
    "\n",
    "print(X_train_count.shape)\n",
    "print(X_validate_count.shape)\n",
    "print(X_d2v_train_np.shape)\n",
    "print(X_d2v_validate_np.shape)\n",
    "\n",
    "import scipy\n",
    "X_merge_train = scipy.sparse.hstack((X_train_count, X_d2v_train_np))\n",
    "X_merge_validate = scipy.sparse.hstack((X_validate_count, X_d2v_validate_np))\n",
    "\n",
    "print(X_merge_train.shape)\n",
    "print(X_merge_validate.shape)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier model..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy = 0.8946"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# model = MLPClassifier(hidden_layer_sizes=(30, 30))\n",
    "model = LogisticRegression(C=100)  # C=0.01\n",
    "# param_grid = {'C': [.0095, 0.01, 0.015]}\n",
    "\n",
    "# param_grid = {'C': [100, 150, 1000]}\n",
    "# grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "# grid.fit(vectorizedComments, Y_words)\n",
    "\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "# print(\"Best parameters: \", grid.best_params_)\n",
    "# print(\"Best estimator: \", grid.best_estimator_)\n",
    "# param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "# print(\"Performing k-fold cross validation for our model...\")\n",
    "# kfoldscores = cross_val_score(logRegModel, X, Y, cv=5)\n",
    "# # print(scores)\n",
    "# print(\"Mean model accuracy = {}\".format(kfoldscores.mean()))\n",
    "print(\"training classifier model...\")\n",
    "model.fit(X_merge_train, Y_train)\n",
    "# predictions = model.predict(X_merge_train)\n",
    "predictions = model.predict(X_merge_validate)\n",
    "accuracy = metrics.accuracy_score(Y_validate, predictions)\n",
    "print(\"Model accuracy = {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "using best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Vectorizing test set\")\n",
    "# vectorizedComments_Z = []\n",
    "# for comment in Z_words:\n",
    "#     vectorizedComments_Z.append(model.infer_vector(comment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# print(\"Running Model on new data\")\n",
    "# predictions = model.predict(Z_d2v)\n",
    "# \n",
    "# f = open(\"predictions_\" + str(int(time.time())) + \".csv\", \"w\")\n",
    "# f.write(\"Id,Category\\n\")\n",
    "# i = 0\n",
    "# for prediction in predictions:\n",
    "#     f.write(str(i) + \",\" + str(int(prediction)))\n",
    "#     f.write(\"\\n\")\n",
    "#     i += 1\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
