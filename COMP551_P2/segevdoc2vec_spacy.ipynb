{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening training and test files..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data frames..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting comments into words and removing html tags..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import importlib\n",
    "import ClassifierDataPrepper\n",
    "importlib.reload(ClassifierDataPrepper)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "dataPath = \"./\"\n",
    "trainingDataPath = dataPath + \"train/\"\n",
    "positiveTrainingDataPath = trainingDataPath + \"pos/\"\n",
    "negativeTrainingDataPath = trainingDataPath + \"neg/\"\n",
    "# testDataPath = dataPath + \"test/\"\n",
    "# positiveTrainingDataPath = trainingDataPath + \"pos_small/\"\n",
    "# negativeTrainingDataPath = trainingDataPath + \"neg_small/\"\n",
    "testDataPath = None\n",
    "print(\"Opening training and test files...\")\n",
    "cdp = ClassifierDataPrepper.ClassifierDataPrepper(positiveTrainingDataPath, negativeTrainingDataPath, testDataPath)\n",
    "\n",
    "print(\"Preparing data frames...\")\n",
    "X, Y = cdp.getXYlabeled()\n",
    "if testDataPath is not None:\n",
    "    Z = cdp.getXtest()\n",
    "print(\"Splitting comments into words and removing html tags...\")\n",
    "X_words = []\n",
    "for comment in X:\n",
    "    comment = cdp.cleanhtml(comment)\n",
    "    comment = comment.lower().split(\" \")\n",
    "    X_words.append(comment)\n",
    "\n",
    "if testDataPath is not None:    \n",
    "    Z_words = []\n",
    "    for comment in Z:\n",
    "        comment = cdp.cleanhtml(comment)\n",
    "        comment = comment.lower().split(\" \")\n",
    "        Z_words.append(comment)\n",
    "    \n",
    "Y_words = []\n",
    "for label in Y:\n",
    "    Y_words.append(label)\n",
    "    \n",
    "print(len(X_words))\n",
    "print(len(Y_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training doc2vec..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"training doc2vec...\")\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = []\n",
    "idx = 0\n",
    "for comment in X_words:\n",
    "    documents.append(TaggedDocument(comment, str(Y_words[idx])))\n",
    "    idx += 1\n",
    "\n",
    "# print(documents)\n",
    "model = Doc2Vec(vector_size=400, min_count=2, epochs=40, workers=8)\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "vectorizedComments = []\n",
    "for comment in X_words:\n",
    "    vectorizedComments.append(model.infer_vector(comment))\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Using spacy tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from data frames..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from data frames...\")\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp_spacy = spacy.load('en', disable=['parser', 'ner'])\n",
    "X_lem = []\n",
    "idx = 0\n",
    "for comment in X:\n",
    "    comment = cdp.cleanhtml(comment)\n",
    "    sentences = nltk.sent_tokenize(comment)  # this gives us a list of sentences\n",
    "    # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "    comment_lem = []\n",
    "    for sentence in sentences:\n",
    "        sentence_lem_tokens = nlp_spacy(sentence)\n",
    "        sentence_lem = \" \".join([token.lemma_ for token in sentence_lem_tokens])\n",
    "        comment_lem.append(sentence_lem)\n",
    "\n",
    "    comment_lem = \" \".join(comment_lem)\n",
    "    # print(\"Comment:\")\n",
    "    # print(comment)\n",
    "    # print(\"Lemmatized Comment:\")\n",
    "    # print(comment_lem)\n",
    "    X_lem.append(comment_lem)\n",
    "    idx += 1\n",
    "    if not idx % 2000:\n",
    "        print(\"processed {} comments\".format(idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 87)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# vect = CountVectorizer(min_df=1, ngram_range=(1, 2), binary=False)\n",
    "# vect = CountVectorizer(ngram_range=(1, 2))\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1, 4))\n",
    "# vect = TfidfVectorizer(min_df=1, ngram_range=(1, 2))\n",
    "\n",
    "# learn the vocabularies from training data for each vector type\n",
    "vect.fit(X_lem)\n",
    "\n",
    "# transform training data\n",
    "X_lem_token = vect.transform(X_lem)\n",
    "\n",
    "# normalize the data (scale it down to 0 -> 1)\n",
    "X_lem_token = preprocessing.normalize(X_lem_token, norm='l2')\n",
    "\n",
    "# model = MLPClassifier(solver='sgd', alpha=1e-5,\n",
    "#                       hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "print(X_lem_token.shape)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished merging docs"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "lem_doc2vec = []\n",
    "for features in vectorizedComments:\n",
    "    merged = features.tolist()\n",
    "    merged += X_lem_token[idx].todense().tolist()[0]        \n",
    "    lem_doc2vec.append(merged)\n",
    "    idx += 1\n",
    "\n",
    "print(\"Finished merging docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(lem_doc2vec))\n",
    "print(len(lem_doc2vec[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.70"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(lem_doc2vec, Y_words)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "using best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Vectorizing test set\")\n",
    "# vectorizedComments_Z = []\n",
    "# for comment in Z_words:\n",
    "#     vectorizedComments_Z.append(model.infer_vector(comment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# print(\"Running Model on new data\")\n",
    "# predictions = grid.best_estimator_.predict(vectorizedComments_Z)\n",
    "# \n",
    "# f = open(\"predictions_\" + str(int(time.time())) + \".csv\", \"w\")\n",
    "# f.write(\"Id,Category\\n\")\n",
    "# i = 0\n",
    "# for prediction in predictions:\n",
    "#     f.write(str(i) + \",\" + str(int(prediction)))\n",
    "#     f.write(\"\\n\")\n",
    "#     i += 1\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
